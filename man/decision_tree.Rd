% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepML.r
\name{decision_tree}
\alias{decision_tree}
\alias{decision_tree.formula}
\alias{treeheight}
\alias{treedepth}
\alias{decision_tree.default}
\alias{is.decisiontree}
\title{Decision Tree}
\usage{
decision_tree(object, ...)

\method{decision_tree}{formula}(formula, data, maxdepth = 100L, ...)

treeheight(node)

treedepth(node)

\method{decision_tree}{default}(x, y, maxdepth = 100L, ...)

is.decisiontree(object)
}
\arguments{
\item{object}{R object.}

\item{...}{Optional arguments.}

\item{formula}{A model \code{\link[stats]{formula}}.}

\item{data}{A data frame, containing the variables in \code{formula}. Neither a matrix nor an array will be accepted.}

\item{maxdepth}{The maximum depth of the resulting tree. If this value, default \code{100}, is reached, the algorithm will stop.}

\item{x}{A matrix or data frame with feature values.}

\item{y}{A factor variable with categorical values for \code{x}.}
}
\value{
A list from class \code{decisiontree} with split nodes and leaf nodes.
}
\description{

}
\details{
A decision tree is a type of model that puts a certain feature from \code{x} onto a node, called split node, of the tree structure on the basis of
  operations (e.g. gini impurity, information gain) and also uses a calculated value of the feature for each node for further separations into
  left and right subnodes. At the end of the tree are the leaf nodes, each of which has a resulting level of \code{y}.\cr
  \code{treeheight()} computes the height of a tree. The height of a tree is the number of nodes from the starting node on the path to its deepest leaf node.\cr
  \code{treedepth()} computes the depth of a tree. The depth of a tree is the number of edges or arcs from the starting node on the path to its deepest leaf node.\cr
}
\examples{
  df <- data.frame(Outlook = factor(c("Sunny", "Sunny", "Overcast", "Rain", "Rain", "Rain", "Overcast", "Sunny", "Sunny", "Rain", "Sunny", "Overcast", "Overcast", "Rain")),
                   Temperature = factor(c("Hot", "Hot", "Hot", "Mild", "Cool", "Cool", "Cool", "Mild", "Cool", "Mild", "Mild", "Mild", "Hot", "Mild")),
                   Humidity = factor(c("High", "High", "High", "High", "Normal", "Normal", "Normal", "High", "Normal", "Normal", "Normal", "High", "Normal", "High")),
                   Wind = factor(c("Weak", "Strong", "Weak", "Weak", "Weak", "Strong", "Strong", "Weak", "Weak", "Weak", "Strong", "Strong", "Weak", "Strong")),
                   PlayTennis = factor(c("No", "No", "Yes", "Yes", "Yes", "No", "Yes", "No", "Yes", "Yes", "Yes", "Yes", "Yes", "No")))

  x <- df[, -5L]
  y <- df[[5L]]
  # Build up decision tree
  tree <- decision_tree(as.formula(PlayTennis ~ .), data = df)
  # Compute height and depth of the tree
  treeheight(tree); treedepth(tree)
  # Predict labels of the features
  yhat <- predict(tree, x)
  accuracy(y, yhat)

}
\seealso{
Other Machine Learning: 
\code{\link{cross_validation_split}()},
\code{\link{k_nearest_neighbors}()},
\code{\link{moving_average}()},
\code{\link{naive_bayes}()},
\code{\link{naive_forecast}()},
\code{\link{predict.decisiontree}()},
\code{\link{predict.kmeans}()},
\code{\link{predict.naivebayes}()}
}
\concept{Machine Learning}
