% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepRNN.r
\name{fit.LSTM}
\alias{fit.LSTM}
\title{Fit LSTM model}
\usage{
fit.LSTM(
  X,
  Y,
  timesteps = 1,
  epochs = 100,
  batch_size = c(1, FALSE),
  validation_split = 0.2,
  k.fold = NULL,
  k.optimizer = NULL,
  hidden = NULL,
  dropout = NULL,
  output.activation = "linear",
  stateful = FALSE,
  return_sequences = FALSE,
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("mean_absolute_error"),
  verbose = 1
)
}
\arguments{
\item{X}{A feature data set, usually a matrix or data.frame, returned by \code{get.LSTM.XY}.}

\item{Y}{n outcome data set, usually a vector, matrix or data.frame, returned by \code{get.LSTM.XY}.}

\item{timesteps}{A number or vector of timesteps for \code{X} and \code{Y}. A timestep denotes the number of different periods of the values within one sample.
A feature does always have at least one timestep, but an outcome is either a scalar with one implicit timestep or a sequence with at least two timesteps.
If only one value for \code{timesteps} is given, this value is used for the resampled feature tensor produced by \code{as.LSTM.X} and, 
if \code{return.sequences = T}, also for resampled outcome tensor produced by \code{as.LSTM.Y}. If two values are given, the first value is used
for the resampled feature tensor and the second for the resampled outcome tensor (sequence or multi-step outcome).}

\item{epochs}{The number of epochs.}

\item{batch_size}{A vector with two elements. The first element holds the batch size, the number of samples used per gradient update.
The second element is boolean to indicate whether the batch size is used for input layer too (\code{TRUE}).}

\item{validation_split}{Fraction of the training data used as validation data.}

\item{k.fold}{Number of folds within k-fold cross validation or \code{NULL} if no grid search is desired.}

\item{k.optimizer}{Either \code{min} or \code{max} to indicate which type of quality measuring is used; if \code{NULL} no quality measure is extracted.}

\item{hidden}{A data.frame with two columns whereby the first column contains the number of hidden units 
and the second column the activation function. The number of rows determines the number of hidden layers.}

\item{dropout}{A numeric vector with dropout rates, the fractions of input units to drop or \code{NULL} if no dropout is desired.}

\item{output.activation}{A name of the output activation function.}

\item{stateful}{A boolean that indicates whether the last cell state of a LSTM unit at t-1 is used as initial cell state of the unit at period t (\code{TRUE}).}

\item{return_sequences}{A boolean that indicates whether an outcome unit produces one value (\code{FALSE}) or values per each timestep (\code{TRUE}).}

\item{loss}{Name of objective function or objective function. If the model has multiple outputs, 
different loss on each output can be used by passing a dictionary or a list of objectives.
The loss value that will be minimized by the model will then be the sum of all individual losses.}

\item{optimizer}{Name of optimizer or optimizer instance.}

\item{metrics}{Vector or list of metrics to be evaluated by the model during training and testing.}

\item{verbose}{Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch) determines how the training progress is visualized.}
}
\value{
A list with named elements
  \code{hyperparamter}: A list with named elements \code{features} and \code{output.units}.
  \code{model}: A trained model object with stacked layers.
  \code{avg_qual}: Only if k-fold cross validation is used. A data.frame with two columns whereby the 
                   first columns stores the epoch number and the second column the mean of the underpinned quality metric.
}
\description{
\code{fit.LSTM} is a wrapper function for building and fitting a LSTM model.
}
\seealso{
\code{\link{build.LSTM}}, \code{\link{get.LSTM.XY}},
  \code{\link[keras]{compile.keras.engine.training.Model}}, \code{\link[keras]{fit.keras.engine.training.Model}}.

Other Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM): 
\code{\link{as.LSTM.X}()},
\code{\link{as.LSTM.Y}()},
\code{\link{as.LSTM.data.frame}()},
\code{\link{as.LSTM.period_outcome}()},
\code{\link{as.lag}()},
\code{\link{as.timesteps}()},
\code{\link{build.LSTM}()},
\code{\link{get.LSTM.X.samples}()},
\code{\link{get.LSTM.X.timesteps}()},
\code{\link{get.LSTM.X.units}()},
\code{\link{get.LSTM.XY}()},
\code{\link{get.LSTM.Y.samples}()},
\code{\link{get.LSTM.Y.timesteps}()},
\code{\link{get.LSTM.Y.units}()},
\code{\link{get.LSTM.period_shift}()},
\code{\link{predict.LSTM}()},
\code{\link{start.LSTM.invert_differencing}()}
}
\concept{Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM)}
