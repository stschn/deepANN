% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepMetrics.r
\name{cross_entropy}
\alias{cross_entropy}
\title{Cross entropy}
\usage{
cross_entropy(p, q, base = NULL)
}
\arguments{
\item{p}{A vector of ground truth probabilities (true probability distribution).}

\item{q}{A vector of estimated probabilities (estimated probability distribution).}

\item{base}{A positive or complex number: the base with respect to which logarithms are computed.
Defaults to \code{NULL} is equal to e = \code{exp(1)}.}
}
\value{
Cross entropy.
}
\description{
Cross entropy
}
\details{
Cross entropy quantifies the difference between two probability distributions.
For a binary classification problem, the following equation can be used instead:
\eqn{-sum((p * log(q)) + ((1 - p) * (1 - log(q))))}
}
\examples{
  # multiclass classification
  # each element represents the probability of the k-th class (k = 1,...,3)
  p <- c(0.10, 0.40, 0.50) # ground truth values
  q <- c(0.80, 0.15, 0.05) # estimated values, e.g. given by softmax function
  cross_entropy(p, q)

  # binary classification
  # the complementary probability is (1 - probability)
  p <- c(1)   # ground truth value
  q <- c(0.8) # estimated value
  cross_entropy(p, q)
}
\seealso{
Other Metrics: 
\code{\link{accuracy}()},
\code{\link{dice}()},
\code{\link{entropy}()},
\code{\link{erfcinv}()},
\code{\link{erfc}()},
\code{\link{erfinv}()},
\code{\link{erf}()},
\code{\link{gini_impurity}()},
\code{\link{huber_loss}()},
\code{\link{iou}()},
\code{\link{log_cosh_loss}()},
\code{\link{mae}()},
\code{\link{mape}()},
\code{\link{mse}()},
\code{\link{msle}()},
\code{\link{quantile_loss}()},
\code{\link{rmse}()},
\code{\link{rmsle}()},
\code{\link{rmspe}()},
\code{\link{sse}()},
\code{\link{stderror}()},
\code{\link{vc}()},
\code{\link{wape}()},
\code{\link{wmape}()}
}
\concept{Metrics}
