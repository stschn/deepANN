% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepMetrics.r
\name{accuracy}
\alias{accuracy}
\title{Classification accuracy}
\usage{
accuracy(
  actuals,
  preds,
  type = c("standard", "misclass", "tpr", "tnr", "ppv", "npv", "fnr", "fpr", "fdr",
    "for", "lrplus", "lrminus", "dor", "ts", "f1", "mcc", "fm", "kappa"),
  compound = FALSE,
  na.rm = FALSE
)
}
\arguments{
\item{actuals}{Numeric data (vector, array, matrix, data frame or list) of ground truth (actual) values.}

\item{preds}{Numeric data (vector, array, matrix, data frame or list) of predicted values.}

\item{type}{Denotes the calculated type of accuracy derivative from confusion matrix.}

\item{compound}{A logical value indicating whether the metric score is calculated for each label (default \code{FALSE}) or across all labels (\code{TRUE}).}

\item{na.rm}{A logical value indicating whether actual and prediction pairs with at least one NA value should be ignored.}
}
\value{
The type-specific accuracy score of a classification problem.
}
\description{
Classification accuracy
}
\details{
The following accuracy types are implemented:
  \itemize{
  \item Standard: \eqn{Number of correct predictions / Total number of predictions}
  \item Misclassification error: \eqn{Number of incorrect predictions / Total number of predictions}
  \item TPR (True Positive Rate), also sensitivity, recall or hit rate: \eqn{TP / (TP + FN)}
  \item TNR (True Negative Rate), also specificity or selectivity: \eqn{TN / (TN + FP)}
  \item PPV (Positive Predictive Value), also precision: \eqn{TP / (TP + FP)}
  \item NPV (Negative Predictive Value): \eqn{TN / (TN + FN)}
  \item FNR (False Negative Rate), also miss rate: \eqn{FN / (FN + TP)}
  \item FPR (False Positive rate), also fall-out: \eqn{FP / (FP + TN)}
  \item FDR (False Discovery Rate): \eqn{FP / (FP + TP)}
  \item FOR (False Omission Rate): \eqn{FN / (FN + TN)}
  \item LR+ (Positive Likelihood Ratio): \eqn{TPR / FPR}
  \item LR- (Negative Likelihood Ratio): \eqn{FNR / TNR}
  \item DOR (Diagnostics Odds Ratio): \eqn{LR+ / LR-}
  \item TS (Threat Score), also critical succes index: \eqn{TP (TP + FN + FP)}
  \item F1 score: \eqn{2 * Precision * Recall / (Precision + Recall)}
  \item MCC (Matthews Correlation Coefficient), also phi coefficient: \eqn{TP * TN - FP * FN / \sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))}
  \item FM (Fowlkes-Mallows index): \eqn{\sqrt((TP / (TP + FP)) * (TP / (TP * FN)))}
  \item Kappa statistics: \eqn{(p0 - pe) / (1 - pe)}
  }

  Standard accuracy and misclassification error are mainly used for single-label classification problems, while the others can also be used for multi-label classification problems.
}
\examples{
accuracy(actuals = c(rep("A", 6), rep("B", 6), rep("C", 6)),
         preds = c(rep("A", 4), "B", "C", rep("B", 5), "A", rep("C", 6)),
         type = "standard")

# preds does not cover all categories of actuals
accuracy(actuals = c(rep("A", 6), rep("B", 6), rep("C", 6)),
         preds = c(rep("A", 10), rep("C", 8)),
         type = "tpr")

}
\seealso{
Other Metrics: 
\code{\link{cross_entropy}()},
\code{\link{dice}()},
\code{\link{entropy}()},
\code{\link{erfcinv}()},
\code{\link{erfc}()},
\code{\link{erfinv}()},
\code{\link{erf}()},
\code{\link{gini_impurity}()},
\code{\link{huber_loss}()},
\code{\link{iou}()},
\code{\link{log_cosh_loss}()},
\code{\link{mae}()},
\code{\link{mape}()},
\code{\link{mse}()},
\code{\link{msle}()},
\code{\link{quantile_loss}()},
\code{\link{rmse}()},
\code{\link{rmsle}()},
\code{\link{rmspe}()},
\code{\link{sse}()},
\code{\link{stderror}()},
\code{\link{vc}()},
\code{\link{wape}()},
\code{\link{wmape}()}
}
\concept{Metrics}
