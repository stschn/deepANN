% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepRNN.r
\name{build.LSTM}
\alias{build.LSTM}
\title{Build LSTM architecture}
\usage{
build.LSTM(
  features,
  timesteps = 1,
  batch_size = NULL,
  hidden = NULL,
  dropout = NULL,
  output = c(1, "linear"),
  stateful = FALSE,
  return_sequences = FALSE,
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("mean_absolute_error")
)
}
\arguments{
\item{features}{Number of features, returned by \code{get.LSTM.X.units}.}

\item{timesteps}{The number of timesteps.}

\item{batch_size}{Batch size, the number of samples used per gradient update.
A batch size should reflect the periodicity of the data, see Culli/Pal (2017:211), Culli/Kapoor/Pal (2019:290).}

\item{hidden}{A data.frame with two columns whereby the first column contains the number of hidden units 
and the second column the activation function. The number of rows determines the number of hidden layers.}

\item{dropout}{A numeric vector with dropout rates, the fractions of input units to drop or \code{NULL} if no dropout is desired.}

\item{output}{A vector with two elements whereby the first element determines the number of output units, returned by \code{get.LSTM.Y.units},
and the second element the output activation function.}

\item{stateful}{A boolean that indicates whether the last cell state of a LSTM unit at t-1 is used as initial cell state of the unit at period t (\code{TRUE}).}

\item{return_sequences}{A boolean that indicates whether an outcome unit produces one value (\code{FALSE}) or values per each timestep (\code{TRUE}).}

\item{loss}{Name of objective function or objective function. If the model has multiple outputs, 
different loss on each output can be used by passing a dictionary or a list of objectives.
The loss value that will be minimized by the model will then be the sum of all individual losses.}

\item{optimizer}{Name of optimizer or optimizer instance.}

\item{metrics}{Vector or list of metrics to be evaluated by the model during training and testing.}
}
\value{
A model object with stacked lstm layers, an output dense layer, and optional dropout layers.
}
\description{
\code{build.LSTM} creates a sequential ANN model with stacked lstm layers, an output dense layer, and optional dropout layers.
  For a univariate time series, usually \code{stateful=TRUE} and \code{batch_size=1} with \code{return_sequences = FALSE}.
  For a multivariate time series, usually \code{stateful=FALSE} and \code{batch_size=NULL} with \code{return_sequences = TRUE}.
}
\seealso{
\code{\link{as.LSTM.X}}, \code{\link{get.LSTM.X.units}}, \code{\link{get.LSTM.Y.units}},
  \code{\link[keras]{keras_model_sequential}}, \code{\link[keras]{layer_dense}}, \code{\link[keras]{layer_dropout}}, \code{\link[keras]{layer_lstm}}
  \code{\link[keras]{compile.keras.engine.training.Model}}.

Other Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM): 
\code{\link{as.LSTM.X}()},
\code{\link{as.LSTM.Y}()},
\code{\link{as.LSTM.data.frame}()},
\code{\link{as.LSTM.period_outcome}()},
\code{\link{as.lag}()},
\code{\link{as.timesteps}()},
\code{\link{fit.LSTM}()},
\code{\link{get.LSTM.X.samples}()},
\code{\link{get.LSTM.X.timesteps}()},
\code{\link{get.LSTM.X.units}()},
\code{\link{get.LSTM.XY}()},
\code{\link{get.LSTM.Y.samples}()},
\code{\link{get.LSTM.Y.timesteps}()},
\code{\link{get.LSTM.Y.units}()},
\code{\link{get.LSTM.period_shift}()},
\code{\link{predict.LSTM}()},
\code{\link{start.LSTM.invert_differencing}()}
}
\concept{Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM)}
